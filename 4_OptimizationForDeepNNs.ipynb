{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student: Rodolfo Lerma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Neural Network Training\n",
    "\n",
    "## Machine Learning 530\n",
    "\n",
    "## Stephen Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Introduction\n",
    "\n",
    "Deep neural networks are trained by **learning** a set of weights. The optimal weights are learned by **minimizing the loss function** for the neural network. This minimization is performed using an **optimization algorithm**. Thus, optimization algorithms are an essential component in our neural network tool box.   \n",
    "\n",
    "In this lesson you will become familiar with the basic optimization algorithms used to train deep neural networks, along with their pitfalls. The nonlinear nature of neural networks leads to several serious problems with local gradients. As a result of the multiple nonlinearities the local gradient can exhibit complex behavior. Further, the local gradient can be quite different from the larger-scale global behavior of the loss function gradient. \n",
    "\n",
    "The high dimensionality of the neural network training optimization problems makes detailed understanding of optimization behavior extremely difficult. There is one dimension for each model weight (parameter). Thus, the optimization is performed over a non-linear surface with millions of dimensions. Despite several decades of research much of the measurable progress has been based on empirical experience rather than theory.   \n",
    "\n",
    "****\n",
    "**Note:** To run notebook you must have Keras package installed, In addition you will need the `hdf5` package installed. You can install this package using either anaconda or pip from a command prompt, as follows:\n",
    "\n",
    "`conda install hdf5`\n",
    "\n",
    "Or,\n",
    "\n",
    "`pip install hdf5`\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Local convergence of optimization algorithms\n",
    "\n",
    "In an idea case, an minimization problem is **convex**. By convex, we mean that the gradient always points in the direction of the **global minimum** of the loss function. Unfortunately, with nonlinear optimization problems, like neural network training, there is no guarantee that the minimization problem is convex. Further the loss function can have multiple **local minimum**. \n",
    "\n",
    "Keeping the foregoing in mind, the minimization of the loss function will at least be convex locally around a minimum. To understand the behavior of a loss function around a minimum we can expand it as a second order Taylor series of the change in the weights from optimization step $l$ to $l+1$:\n",
    "\n",
    "$$J(W^{(l+1)}) = J(W^{(l)}) + (W^{(l+1)} - W^{(i)})\\vec{g} + \\frac{1}{2}(W^{(l+1)} - W^{(i)})^T H (W^{(l+1)} - W^{(i)}) $$\n",
    "\n",
    "where,   \n",
    "$W^{(l)}$ is the tensor of weights at step $l$,  \n",
    "$\\vec{g}$ is the gradient vector,  \n",
    "$H$ is the **Hessian** matrix. \n",
    "\n",
    "The Hessian is a matrix of second partial derivatives. You can think of the Hessian as being the rate of change of the gradient or the gradient of the gradient. For a vector gradient $f(\\vec{x})$ the Hessian is:\n",
    "\n",
    "$$H \\big(f(\\vec{x}) \\big) = \\begin{bmatrix}\n",
    "  \\frac{\\partial^2 f(\\vec{x})}{\\partial x^2_1} & \n",
    "  \\frac{\\partial^2 f(\\vec{x})}{\\partial x_2 \\partial x_1} & \n",
    "  \\cdots & \n",
    "  \\frac{\\partial^2 f(\\vec{x})}{\\partial x_n \\partial x_1}\\\\\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x_1 \\partial x_2} &\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x^2_2} & \n",
    "   \\cdots &\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x_1 \\partial x_n}\\\\\n",
    "   \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x_1 \\partial x_n} &\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x_2 \\partial x_n} & \n",
    "   \\cdots &\n",
    "   \\frac{\\partial^2 f(\\vec{x})}{\\partial x^2_n} \n",
    " \\end{bmatrix}$$\n",
    " \n",
    "The Hessian has several useful properties. \n",
    "\n",
    "- The Hessian is symmetric, since $\\frac{\\partial^2 f(\\vec{x})}{\\partial x_1 \\partial x_2} = \\frac{\\partial^2 f(\\vec{x})}{\\partial x_2 \\partial x_1}$.\n",
    "- If the eigenvalues of the Hessian are all positive, the curvature of the gradient is upward, indicating and minimum point in $f(\\vec{x})$. The optimization is convex, at least locally. In this case we say the Hessian is **positive definite**.\n",
    "- If the eigenvalues of the Hessian are all negative, the curvature of the gradient is downward, indicating and maximum point in $f(\\vec{x})$. In this case we say the Hessian is **negative definite**.\n",
    "- A Hessian with mixed sign eigenvalues indicates gradient with upward curvature in some dimensions and downward curvature in other dimensions. This situation with mixed curvature is known as a **saddle point**. \n",
    "- For a Gaussian process the Hessian is the inverse of the covariance matrix. The eigenvalues of each matrix are just the inverse of the other. \n",
    "\n",
    "For a step size $\\alpha$ we can rewrite the first equation as:\n",
    "\n",
    "$$J(W^{(l)}- \\alpha \\vec{g}) = J(W^{(l)}) - \\alpha \\vec{g}^T \\vec{g} + \\frac{1}{2} \\alpha^2 \\vec{g}^T H \\vec{g}$$\n",
    "\n",
    "The minimum point of $J(W^{(l)}- \\alpha \\vec{g})$ occurs were the gradient is zero in all dimensions. This is evident from the fact that at this point:\n",
    "$$J(W^{(l)}) = J(W^{(l)}- \\alpha \\vec{g})$$ \n",
    "\n",
    "From this point no further reduction in the loss function is possible. The optimal step size for the quadratic approximation is then: \n",
    "\n",
    "$$\\alpha^* = \\frac{\\vec{g}^T \\vec{g}}{\\vec{g}^T H \\vec{g}}$$\n",
    "\n",
    "But, what happens if the Hessian is not well behaved? One measure of 'behavior' for a Hessian is the **condition number**:\n",
    "\n",
    "$$\\kappa(H) = \\frac{|\\lambda_{max}(H)|}{|\\lambda_{min}(H)|}$$\n",
    "\n",
    "where,  \n",
    "$|\\lambda_{max}(H)|$ is the absolute value of the largest eigenvalue of H.  \n",
    "$|\\lambda_{min}(H)|$ is the absolute value of the smallest eigenvalue of H.   \n",
    "\n",
    "The condition number of the Hessian has serious implications for the rate of convergence of optimization algorithms. If the condition number is small, the Hessian is well conditioned and the gradient has similar scale in all dimensions. This happy situation leads to rapid convergence. An ideal condition number is close to 1. \n",
    "\n",
    "However, if the Hessian is **ill-conditioned**, having a large condition number, the scale of the gradient will be quite different in different dimensions. An optimization algorithm will converge quickly along eigenvector directions corresponding to large eigenvalues. However, convergence will be slow along the direction of eigenvectors with small eigenvalues. This situation has been describe as slowly meandering down a long narrow valley. In fact, for real-world stochastic problems (e.g. noisy data), the optimization algorithm may not converge at all along some eigenvector directions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Multimodal loss function and global optimization\n",
    "\n",
    "The nonlinear nature of the hidden layers can lead to loss functions with significant local structure in a high dimensional space. Given this complexity, it is quite possible there are local minimum, local maximum, or saddle points. In general, there is no guarantee that the global minimum of the objective function can ever be found.     \n",
    "\n",
    "In the early days of neural network research it was generally thought that loss function minimization got 'stuck' at local minimum or saddle points. However, recent experience indicates that this may not be the case. In many real-world cases, the training loss function continues to decrease with epochs. If the optimization algorithm were stuck, this could not be the case. We have explored this behavior in previous lessons.    \n",
    "\n",
    "Continued convergence of the optimization process does not mean that convergence will be rapid. Empirical experience indicates that slow convergence is a common problem. This situation occurs when the Hessian of the loss function is ill-conditioned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Vanishing and exploding gradients\n",
    "\n",
    "Some common pitfalls of deep neural network loss functions are **vanishing gradients** and **exploding gradients**. Vanishing gradients arise when multiple small gradients are encountered in backpropagation. Exploding gradients arise when rapid changes in the loss function, sometimes referred to as **cliffs**, are encountered in the loss function.   \n",
    "\n",
    "A deep linear model analogy can aid in understanding vanishing gradients. In this simple model each layer has the same weights, represented by the tensor $W$. We can compute an eigenvalue-eigenvector decomposition of $W$:\n",
    "\n",
    "$$W = Q \\Lambda Q^T$$   \n",
    "where,   \n",
    "$Q$ is the unitary eigenvector matrix,   \n",
    "$\\Lambda$ is the diagonal matrix of eigenvalues. \n",
    "\n",
    "At the nth layer a signal entering the top of the network will be weighted by $W^n$ which we can write:\n",
    "\n",
    "$$W = \\big( Q \\Lambda Q^T \\big)^n = Q \\Lambda^n Q^T$$\n",
    "\n",
    "In order to have a stable network all the eigenvalues must be less than 1. Therefore $\\Lambda^T$ is a diagonal matrix of increasingly small numbers as $n$ increases. The net effect is that gradients from deep in the networks can be exponentially smaller than from shallow layers. When the backpropagation is applied, the gradient effectively vanishes toward 0. \n",
    "\n",
    "Exploding gradients arise from sudden changes in curvature of the loss function. Encountering these 'cliffs' results in a gradient decent algorithm overshooting the minimum point, sometimes by an extreme amount. The Hessian represents the curvature of the loss function or the rate of change of the gradient. The eigen-decomposition of the Hessian:\n",
    "\n",
    "$$H(J(W)) = Q \\Lambda Q^T = Q diag(\\lambda) Q^T$$\n",
    "\n",
    "Consider what happens when the loss function has high local curvature. At one optimization step, the eigenvalues $diag(\\lambda)$ are all small and well behaved. The Hessian has a small condition number. At the next step the eigenvalues can become enormous (much greater than 1), since the curvature of the loss function is changing so rapidly. Since only some eigenvalues grow large, the condition number becomes extremely large.  This leads to the exploding gradient! \n",
    "\n",
    "Fortunately, there is a simple solution the exploding gradient problem, **gradient clipping**. As the name implies, gradient clipping is nothing more than imposing a hard maximum constraint on the gradient. In practice, this simple algorithm have proven to be quite effective. \n",
    "\n",
    "***\n",
    "**Note:** All optimizers in Keras have parameters to clip individual weights or the norm of the gradient. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Flat spots\n",
    "\n",
    "With complex loss function it is common to have regions that are **flat**. These reasons are often called **plateaus**. In other words, areas with negligible gradient. These regions can result in extremely slow learning. There are a number of solutions for this problems. The learning rate can be increased or momentum can be used. These approaches are discussed later in this lesson. \n",
    "\n",
    "****\n",
    "**Note:** Keras has a callback that can be used to take action when slow learning is encountered.  \n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.0 Batch gradient decent for backpropagation\n",
    "\n",
    "Recall, that we train neural networks using the **backpropagation** algorithm. The backpropagation algorithm requires several components:\n",
    "\n",
    "1. A **loss function** to measure how well our representation matches the function we are trying to learn. \n",
    "2. A method to propagate changes in the representatuion (weights) through the complex network For this we will use the **chain rule of calculus** to compute **gradients** of the representation. In the general case, this process requires using automatic differentiation methods. \n",
    "3. An **optimization algorithm** that uses the gradients to minimize the loss function.  \n",
    "\n",
    "The backpropagration algorithm learns the optimal weights for the neural network by taking small steps in the direction of the **local gradient**. By *local gradient* we mean the gradient of $J(W)$ computed at each set of weights $W$ as the algorithm proceeds. \n",
    "\n",
    "Once we have the gradient of the loss function we can update the tensor of weights using the formulation below.\n",
    "\n",
    "$$W_{t+1} = W_t + \\alpha \\nabla_{W} J(W_t) $$  \n",
    "where  \n",
    "$W_t = $ the tensor of weights or model parameters at step $t$.   \n",
    "$\\alpha\\ = $ step size or learning rate.  \n",
    "$J(W) = $ loss function given the weights.  \n",
    "$\\nabla_{W} J(W) = $ gradient of $J$ with respect to the weights $W$.  \n",
    "\n",
    "It should be evident that the back propagation algorithm is a form of gradient decent. The weights are updated in small steps following the local gradient of $J(W)$ down hill. At the **termination condition** $J(W)$ should be at or very near the minimum possible value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compuational example\n",
    "\n",
    "The basic idea is simple, but actually optimizing a complex neural network is another mater altogether. To demonstrate the concept, we will work on a very simple 2-d problem. The loss function in this case is the mean square error (MSE). So, in effect, the minimum MSE is the same as the maximum likelihood (MLE) solution. \n",
    "\n",
    "The loss function for a Gaussian process is:\n",
    "\n",
    "$$J(\\hat{x}) = \\frac{1}{N} \\sum_{i = 1}^{N} \\big( \\vec{x}_i - \\hat{x} \\big)^2$$  \n",
    "\n",
    "where;  \n",
    "$x = $ the sample data, which is a 2d tensor in this case of dimension $N x 2$ where $N$ is the number of samples,    \n",
    "$\\hat{x} = $ the vector of means we want to estimate.  \n",
    "\n",
    "We can compute the gradient for each dimension of as follows:\n",
    "\n",
    "$$\\frac{ \\partial \\hat{x}}{ \\partial x_j} = \\frac{2}{N} \\sum_{i = 1}^{N} \\big(  x_{ij} - \\tilde{x}_j \\big)$$  \n",
    "\n",
    "where,  \n",
    "$x_j = $ the jth dimension of $\\vec{x}$,   \n",
    "$x_{ij} = $ the ith component of the jth dimension of $\\vec{x}$,   \n",
    "$\\tilde{x}_i = $ is the current estimate of ith component of $\\hat{x}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to load the packages required to execute the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import keras.utils.np_utils as ku\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import numpy.linalg as nll\n",
    "import sklearn.model_selection as ms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below simulates a bivariate Normal distribution with high covariance between the two dimensions. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = np.array([[1.0, 0.999], [0.999, 1.0]])\n",
    "mean = np.array([1.0, 2.0])\n",
    "\n",
    "sample = nr.multivariate_normal(mean, cov, 500)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, for a Gaussian process, the covariance matrix is the inverse the Hessian. This means that both matrices have the same condition number. Execute the code in the cell below to compute and display the eigenvalues and condition number of this model data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues = [1.999e+00 1.000e-03]\n",
      "The condition number = 1998.9999999997765\n"
     ]
    }
   ],
   "source": [
    "eigenvalues = nll.eig(cov)[0]\n",
    "print('Eigenvalues = ' + str(eigenvalues))\n",
    "print('The condition number = ' + str(eigenvalues[0]/eigenvalues[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix has a high condition number. This optimization problem will deliberately strain the algorithms. \n",
    "\n",
    "****\n",
    "**Note:** In a real-world problem, the condition number can be improved by simple Z-Score scaling. However, for the purpose of demonstration we will skip this step. \n",
    "****\n",
    "\n",
    "Next, execute the code in the cell below to plot the simulated data and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3df5xcdX3v8dd7l4nZsOBCwUUWMEq9sWAEblaNjW13qQX8FTEFkYf0tr1t87g/1FpiLFQqoZUSb4oXq72PSqvVChJ+7+WHGKCwcqWCTdyEEDFqRYWBBqwssLDCJvncP2Ym7E5mZ2d3zuyZOfN+Ph77YPfMmTOfL5v9fs75nu/5fBURmJlZ++lIOwAzM0uHE4CZWZtyAjAza1NOAGZmbcoJwMysTTkBmJm1KScAszmStE7SFQkd60uSPpnEscxq5QRgLUfSWyX9i6SnJf1c0r2S3ph2XPNF0rCkP0w7Dmt9B6QdgNlsSDoYuAX478A1wALg14AX0ozLrBX5CsBazX8CiIirImJPRIxHxO0R8QCApGMl3SXpPyT9TNKVknpKb5b0Y0lrJT0g6TlJX5DUK+k2Sc9KulPSIcV9F0sKSaslPSbpcUlrpgtM0vLilcmopG2SBqrse5Kk7xQ/82pg4aTXDpF0i6QnJT1V/P6o4msXU0h4n5M0Julzxe2fkfSIpGckbZH0a/X8T7b24ARgreb7wB5JX5b09lJnPYmAS4AjgV8BjgbWle3z28BvUUgm7wZuA/4MOIzC38SHy/YfBF4LnAKcJ+lt5UFJ6gNuBT4JHAp8FLhe0uEV9l0ADAFfKe57bTGmkg7gH4FXAccA48DnACLi48D/Az4YEd0R8cHie/4VOLF4vK8C10paiFkVTgDWUiLiGeCtQAB/Dzwp6SZJvcXXfxgRd0TECxHxJPBp4DfKDvPZiNgVEXkKnen9ETESES8ANwInle1/UUQ8FxHbKXTMZ1cI7RzgaxHxtYjYGxF3AJuBd1TYdzmQAy6LiImIuI5CB15q439ExPUR8XxEPAtcXKEN5f9frii+b3dEXAq8DFhS7T1mTgDWciLioYj4vYg4Cng9hbP9ywAkvULSRkl5Sc8AV1A4s59s16Tvxyv83F22/yOTvv9J8fPKvQo4szj8MypplEKiemWFfY8E8jG1EuNPSt9IWiTp85J+UmzDPUCPpM4Kxyq9Z42kh4o3xkeBl7N/u82mcAKwlhYR3wO+RCERQGH4J4A3RMTBFM7MVefHHD3p+2OAxyrs8wjwlYjomfR1YESsr7Dv40CfpMlxHTPp+zUUzt7fXGzDrxe3l/afUsK3ON7/p8D7gEMiogd4mvrbbRnnBGAtRdLrime7pZuiR1MYkrmvuMtBwBgwWhyXX5vAx/558az8eOD3gasr7HMF8G5Jp0rqlLRQ0kApzjLfAnYDH5Z0gKRVwJsmvX4QhSuRUUmHAheWvX8X8Jqy/XcDTwIHSPoEcPAc2mltxgnAWs2zwJuB+yU9R6Hjf5DCWTPARcB/pnAGfCtwQwKf+Q3gh8A/A38dEbeX7xARjwDvoXAz+UkKVwRrqfA3FhEvAquA3wOeAs4qi/MyoAv4GYX2fb3sEJ8BzijOEPobYBOFG9nfpzCU9AumDluZVSQvCGNWmaTFwMNALiJ2pxuNWfJ8BWBm1qacAMzM2pSHgMzM2pSvAMzM2lRLFYM77LDDYvHixVX3ee655zjwwAPnJ6AGy1JbIFvtyVJbIFvtcVv2t2XLlp9FxH5lSVoqASxevJjNmzdX3Wd4eJiBgYH5CajBstQWyFZ7stQWyFZ73Jb9SfpJpe0eAjIza1NOAGZmbcoJwMysTTkBmJm1KScAM7M25QRgZtamWmoaqJlZOxkdn2DF+rt4bHScI3u6WHvqEk4/qS+x46eaACT9mEJ53z3A7ojoTzMeM7NmMTSSJ//UOPnRwkJw+dFxzr9hO0BiSaAZhoAGI+JEd/5mZi/ZsGkne8tqtY1P7GHDpp2JfUYzJAAzMyvz2Oj4rLbPRarVQCU9TGFFpAA+HxGXV9hnNbAaoLe3d9nGjRurHnNsbIzu7vI1vVtTltoC2WpPltoC2WpPVtqy89+f5ZAFe9lV1t8v6OxgyREHzepYg4ODWyqNsqSdAI6MiMckvQK4A/hQRNwz3f79/f3hWkCtK0vtyVJbIFvtyUpbhkby5B/awoYHOvdt68p1csmqpbO+ByCpYgJIdQgoIh4r/vcJ4EamLoxtZta2Tj+pj75Duujr6UJAX0/XnDr/alKbBSTpQKAjIp4tfn8K8BdpxWNm1mx6unLce95Aw46f5jTQXuBGSaU4vhoRX08xHjOztpJaAoiIHwEnpPX5ZmaNMDSSZ8OmnQ17eCtJfhLYzCwhQyN5zr9hO+MTe4DGPLyVJD8HYGaWkA2bdu7r/EuSfngrSU4AZmYJmY+Ht5LkBGBmlpAje7pmtT1tTgBmZglZe+oSch2asi3XIdaeuiSliKpzAjAzS5Jm+LmJOAGYmSVkw6adTOyZWl5nYk807U1gTwM1M5ul6eb6t9pNYCcAM7NZqDbX/8ieLvIVOnvfBDYzy4Bqc/3XnrqErlznlNe6cp1NexPYVwBmZlWUD/dUOsOHwjBP6Wlfl4IwM2txFwxt58r7fkrptm5+dBwBlVZRKQ3znH5SX9N2+OU8BGRmVsHQSH5K518S7D+zs5mHeapxAjAzq2DDpp0Vz/ShkAQauVDLfPEQkJlZ0eTx/mqL5fb1dHHveSfPW1yN4gRgZsb+0zunI2jJ4Z5KPARkZkbl6Z3lBHxg+TEtOdxTia8AzMyo/rSuoOmndM6FE4CZGdM/xZuV8f5KPARkZgYt9xRvEnwFYGYGLfcUbxKcAMws06ar3FlJKz3FmwQnADPLrGqVO9upo5+O7wGYWWZVq9xpTZAAJHVKGpF0S9qxmFm2tNoCLfMt9QQA/DHwUNpBmFn2TLcQS7Mu0DLfUk0Ako4C3gn8Q5pxmFk2tePUztlQRLWSRw3+cOk64BLgIOCjEfGuCvusBlYD9Pb2Ltu4cWPVY46NjdHd3d2AaOdfltoC2WpPltoCrdWe0fEJdj39C17cs5cFnR30vnwhPV25fa+Xt2Wm/ZtZUr+XwcHBLRHRX749tVlAkt4FPBERWyQNTLdfRFwOXA7Q398fAwPT7grA8PAwM+3TKrLUFshWe7LUFmid9gyN5Dn/n7czPtFBaQCjK7eHS1Ydt29WT6u0pRaNbkuaQ0ArgJWSfgxsBE6WdEWK8ZhZk1t30w7P6klQagkgIs6PiKMiYjHwfuCuiDgnrXjMrHkNjeQ57s9vY3R8ouLrntUzN34QzMya1tBInnU37Zi24y/xrJ65aYoEEBHDwHDKYZhZEylfkL0az+qZm2Z4DsDMbIrpFmSv5JBFOZd1mKOmuAIws/ZSXqBt8HWHc/f3ntz38/Mv7q6p8xdw4buPb3S4meUEYGbzqlKBtivu++m+1ystyjKdLC3PmAYnADObV7WsvTuTnq4c61Ye786/Tk4AZjav6p2y+eP170woEvNNYDObVz2L5l6Goc/TPRPlBGBm82ZoJM9Tz1ef0z8dF3FLnoeAzCwxM83uee6F3XM67iGLclz4bo/5J80JwMwSkdTsng7BwQtzPD0+0RYLs6fJCcDMEpHE7B6AvQEHvuwAtl54SgJRWTVOAGZWl9Kwz2zm78/Exd3mhxOAmc1Z+bBPUlzcbX54FpCZzVlSwz6TebbP/HECMLM5GRrJJzLsc87yY+jr6UIU5vlfsmqpb/rOEw8BmdmslYZ+6rXi2EP55OlLE4jI5sIJwMxqVusCLbVYceyhXPlHb0kgKpsrJwAzq8nQSJ61125jYm8thZqn1yH49PtO9DBPE/A9ADOryYZNO+vu/HOdcuffRJwAzKwmc7nhW36Dd8MZJ7jzbyIeAjKzqoZG8lx0845Zv6+nK+cbvE3OCcDMprhgaDtX3f8IeyKQgKCm5RknE7BupZdqbHZOAGa2zwVD26cUcIs5DPkLL9XYKpwAzNrcBUPbOeK5Z/i9826t+1h9rt7ZUpwAzNrM5Jr9C3MdjE/sZc3S+mf3+AZv63ECMGsj5cXbxif21n1ML9bSulJLAJIWAvcALyvGcV1EXJhWPGbtYN1NOxIp3uZOPxvSvAJ4ATg5IsYk5YBvSrotIu5LMSazzLpgaHvdJRx6unKsW+mOPytSSwAREcBY8cdc8au+gUgzq+gDf/8t7v23n8/5/b65m02KuczzSurDpU5gC/DLwN9GxJ9W2Gc1sBqgt7d32caNG6sec2xsjO7u7gZEO/+y1BbIVntaoS2j4xPsevoXvLhn5nH+3i7YNc2Dvgs6O1hyxEEJR9c4rfC7qVVSbRkcHNwSEf3l26dNAJIOBs4HjgJui4ivTnrt/0TE/6g7qpeO1wPcCHwoIh6cbr/+/v7YvHlz1WMNDw8zMDCQVGipylJbIFvtafa2XDC0nSvv+2nNl9Rrlu7m0u37Dwi0YuG2Zv/dzEZSbZFUMQFUqwX0jxSe6bgeeL+k6yW9rPja8rojmiQiRoFh4LQkj2vWjoZG8rPq/KezKNfRcp2/zU61ewDHRsRvF78fkvRx4C5JK5P4YEmHAxMRMSqpC3gb8Kkkjm3WzjZs2llX53/ZWe7020W1BPAySR0RsRcgIi6W9CiFqZtJDLC9Evhy8T5AB3BNRNySwHHN2k6pYNtTz9c3y+ccl3BoK9USwM3AycCdpQ0R8WVJu4DP1vvBEfEAcFK9xzFrV/V2+rkO0b3wAEafn2BBZ4fP/NvQtPcAIuJjEXFnhe1fj4jXNjYsM6tmaCTP2uu21XXGP7E3WLTgAB5e/06WHHGQO/825FIQZi1maCTPmmu2sSeBKdyPzWGRF8sOJwCzJlcq3pYfHadYnj8xR/Z0JXg0azVOAGZNrLx4W5Kdf1euk7WnLknwiNZqakoAkn4VWDx5/4j4pwbFZGZFGzbtTKR4WzmXdjCoIQFI+gpwLLAVKP1LDMAJwKxBkprWCewbNnKnb+VquQLoB46LNIsGmbWR0gyfiT31/8m5bLNVU0sCeBA4Ani8wbGYGfBnNzxQd+fvss1Wi1oSwGHAdyV9m0INfwAiIpGSEGaW3JDPgQs6ufi9S93xW01qSQDrGh2EWbtJempnrkNsONNr8trszJgAIuIbknqBNxY3fTsinmhsWGbZNDSSZ91NO6aszJXEzTV3/jYX1cpBAyDpfcC3gTOB9wH3Szqj0YGZZU1pTn+9yzKW6+vpcudvc1LLENDHgTeWzvqLZZzvBK5rZGBmWdOIOf1+mMvqUUsC6Cgb8vkParhyMGt3pXH+x0bHObKni3yCdXdEoYyD5/VbPWpJAF+XtAm4qvjzWcDXGheSWesbGsmz9tptTOwtjPAn2fn3dOXYeuEpiR3P2lctN4HXSvptYAWFE4/LI+LGhkdm1sLW3bRjX+efpFyHWLfy+MSPa+2pplpAEXE9hbWBzWwGQyP5RG/0dgr2hod8LHnTJgBJ34yIt0p6lqkz1QRERBzc8OjMWkxppk8SOiXOfvPRfPL0pYkcz6zctAkgIt5a/O9B8xeOWWspv9H71HMvMD6xt+7jnrP8GHf81nC1PAdwrKSXFb8fkPRhST2ND82suZXO9vOj4wSFG73PJ9D5rzj2UHf+Ni9qmc55PbBH0i8DXwBeDXy1oVGZNamhkTwr1t/F9vzTnHvN1kTn9ff1dHHZWSdy5R+9JbFjmlVTy03gvRGxW9J7gcsi4rOSRhodmFmzmbI619GFG7NJWJTr4Lt/+fZkDmY2C7VcAUxIOhv4XeCW4rZc40Iya07rbtqR+JO8uU7xV6vekOgxzWpVSwL4feAtwMUR8bCkVwNXNDYss+aS9NROKAz5bDjDRdwsPbU8CPZd4MOTfn4YWF/vB0s6msKykkcAeyk8YPaZeo9rlqRK1TuTIODe805O9Jhms1XLmsArKKwJ8Kri/qXnAF5T52fvBtZExHckHQRskXRHMeGYpa68nEOSjuzpSvyYZrNVyxDQF4BPA2+lsCZAPy+tDTBnEfF4RHyn+P2zwEOAr4WtaWzYtLPuzv+c5cfQleucss0VPK1ZaKa13iXdHxFvbmgQ0mLgHuD1EfFM2WurgdUAvb29yzZu3Fj1WGNjY3R3dzcm0HmWpbZA67Vne/7paV/r7YJdNdR3W9r3ckbHJ9j19C94cc9eFnR20PvyhfR0Ndc8ilb73VTjtuxvcHBwS0T0l2+vJQGsBzqBG5i6JvB36o6qcPxu4BsUbjLfUG3f/v7+2Lx5c9XjDQ8PMzAwkERoqctSW6C521P+RO/g6w7nivt+Ou3+a5bu5tLt1UdQ+3q6Wmacv5l/N7PltuxPUsUEUMtzAKWz/8lvDqDuf9mSchQeNLtyps7frFGmzO+n8ERvtc6/FgIP81jTq2UW0GAjPliSKNxfeCgiPt2IzzCrxUU3Jz+//wPLj/H0Tmt6tdQC6pX0BUm3FX8+TtIfJPDZK4DfAU6WtLX49Y4EjmtWsw/8/bd46vnkpngKF3Kz1lHLENCXgH+ksDYwwPeBqymcvc9ZRHyTwt+LWcOVj/GvPXUJ127+Kff+28/nfMxFuQ5WLTuKu7/35JTj+szfWkUtCeCwiLhG0vkAxbpAyV4vmzVQpTH+j1y9dc7HE3D0oYv47l/+VkIRmqWjlucAnpP0SxQXhZG0HJh+fpxZk2lEDZ9mm8ZpNhe1XAGcC9wEHCvpXuBw4IyGRmWWkEbU8PFTvJYVtcwC+o6k3wCWULj63RkRyf5FmTXIx67blujx9j3F+/QPEj2uWRpqmQXUCbwD+E3gFOBDks5tdGBm9RoayfPinvpKOQg4ZFEOUXiw65JVS32T1zKjliGgm4FfANspVO00azrls3wW/1JXXTN8oND5/++zTnSHb5lVSwI4KiK8YoU1rUqzfPKjNRTqqUL4YS7LvloSwG2STomI2xsejdkcbNi0M5FZPqIw1a3P8/mtTdSSAO4DbpTUAUzw0noABzc0MrMZDI3kuejmHYk8yXvIohwXvvt4d/rWVmpJAJdSWBJye8xUOtRsngyN5Fl73TYm6rzJC4XOf+QTpyQQlVlrqSUB/AB40J2/pa0RyzPmOsSF7z4+seOZtZJaEsDjwHCxGNzk9QBcwdPmzdBInnOv3proNLSerhzrVnrYx9pXLQng4eLXguKX2bwpTe+sd1ZPSa5TbDjjBHf6ZtT2JPBF8xGIWbny6Z318o1es6mmTQCSLouIj0i6mWIhuMkiYmVDI7O2NjSS50+u3rr/P7w5eO0rDuSOcwcSOJJZtlS7AvhK8b9/PR+BmJUMjeRZe+22ujv/H69/ZyLxmGXVtAkgIrYU//sNSYcXv39yvgKz9pHkfH4zq920xeBUsE7Sz4DvAd+X9KSkT8xfeJZ1pfn8SXf+fS7ZbDajatVAP0Jh3d43RsQvRcQhwJuBFZL+ZF6is8y76OYdiTzMNdm+ks1mVlW1BPBfgLMj4uHShoj4EXBO8TWzugyN5BM58891yCWbzeag2k3gXET8rHxjRDwpyevhWV2GRvKsuab+xVo6JTac6Xn9ZnNRLQG8OMfXzKq6YGg7V9z307qP05Xr9Nm+WR2qJYATJD1TYbuAhQ2KxzJsdHyCk/7i9kSGfTold/5mdao2DbRzPgOx7Cqd8a9Zupunnq+l+shUXbnOKU8D+8zfLBmz/2s0q8EFQ9u56v5H2JNAEdlLVi2dstyjF2sxS0aqCUDSF4F3AU9ExOvTjMWSk9QYP0BXroPTT+pzh2/WANWmgc6HLwGnpRyDJezKhDr/DuCSVV6O2qxRUr0CiIh7JC1OMwarXyNKObhWv1njKe2FvooJ4JbphoAkrQZWA/T29i7buHFj1eONjY3R3d2dcJTpaIW2jI5P8OhT49Ty76i3C3bNUNZ/QWcHvS9fSE9Xcz9q0gq/m9nIUnvclv0NDg5uiYj+8u1NfxM4Ii4HLgfo7++PgYGBqvsPDw8z0z6tohXasmL9XeRHa5swtmbpbi7d/tI/uQMXdPL8i3ta8sZuK/xuZiNL7XFbatf0CcCa22NzXKnrnOXH8MnTlyYcjZnNhhOAzVq9UzwvO+vEljrbN8uqVGcBSboK+BawRNKjkv4gzXhsZqUpnnPt/M9Zfow7f7MmkfYsoLPT/HybXmkx9vKHr666/5E5He+QRTmOPnQBH3q7h33MmkXazwFYEyotxp4fHSeA/Og4f3L1Vhafd+uczvzPWX4MI584peln9pi1G98DsP1s2LRzSu0dYE7r83ouv1lzcwKw/eTnOLNnMi/Ibtb8PARk++mU6nq/h3rMWoMTgO2nngqeHcC6lccnF4yZNYyHgCyxWj4e8zdrLU4AbW5oJM+aa7exZ+/cz/r9VK9Za3ICaFOlef713PDta8EaPmb2EieANlSa518+1XM6XpLRLJucADKu0hO9leb5T6dv0nu8JKNZtjgBZFj5mX5+dHxWZ/65Du3r7N3hm2WPE0BGTbcub62d/6JcB3+16g3u+M0yzAkgg+pdlN3lms3agx8Ey6B6FmXv6cq58zdrE04AGTM0kp9T4TYojPn7KV6z9uEhoAwZGslz7jVb5/Rez+k3az9OABkxNJJn7XXbmO0DvZ7Tb9a+nABaXD1P9B64oJOL3+vO36xdOQG0sNk+0TvZimMP5co/eksDojKzVuEE0GImP9nbIc26dHOnxKXvO8Fn/WbmBNBKys/4Z9v5e7zfzCZzAmghs6nhA9CV62BhrpPR5ydcw8fM9uME0EIeq+FGr8/yzaxWfhCsCQ2N5Fmx/i62559mxfq7GBrJA3BkT1fF/TslRGEuvzt/M6tVqlcAkk4DPgN0Av8QEevTjKcZTBnnP/qlCp4Aa09dst+sH5/xm9lcpXYFIKkT+Fvg7cBxwNmSjksrnmZRaZx/fGIPGzbt5PST+rhk1VL6erp8xm9mdUvzCuBNwA8j4kcAkjYC7wG+m2JMqZtunL+03bX5zSwpillOJUzsg6UzgNMi4g+LP/8O8OaI+GDZfquB1QC9vb3LNm7cWPW4Y2NjdHd3NyboBhodn2DX07/gxT17923r7YJdxXywoLODJUcclFJ0yWjV300lWWoLZKs9bsv+BgcHt0REf/n2NK8AVGHbftkoIi4HLgfo7++PgYGBqgcdHh5mpn3SVL5E4+DrDufWBx7nqecnKIzIvTQqt2bpbi7dfgC5DrHhzBMYaPEz/2b/3cxGltoC2WqP21K7NBPAo8DRk34+CngspVjmRaUlGmtZuKV74QEe9jGzxKU5DfRfgddKerWkBcD7gZtSjKfhZvsgV8no8xMNiMbM2l1qVwARsVvSB4FNFKaBfjEidqQVz3yo5UGuSqab/29mVo9UnwOIiK8BX0szhkYqH+/vWZQrjvXXrivXydpTlzQoQjNrZy4F0SCVxvtzHSLXKSb21DbzakFnh+f5m1nDuBREg1Qa75/YGxy44IApD3KtOPbQ/aZDdeU6ueysE1lyxEHu/M2sYXwF0CDTjfc/PT7B1gtPmbKtfKioVLVzePgH8xGqmbUpJ4AGObKnq+IyjZVu6PrpXjNLg4eAGmTtqUvoynVO2eYbumbWTJwAElIq4fzq825lxfq7AFy4zcyamoeAElBpxs/5N2znklVLufe8k1OOzsysMl8BJKBaCWczs2blBJCAmUo4m5k1IyeABExXqsElHMysmTkBJMAzfsysFfkmcAXTPZg1ndJrs3mPmVnanADKTDejB5gxCbjDN7NW4iGgMp7RY2btwgmgjGf0mFm7cAIo4xk9ZtYu2vYewNBInotu3rFvgZaerhzrVh7P2lOXTLkHAJ7RY2bZ1JZXAEMjedZet23K6lyj4xOsvXYb4Bo+ZtYe2vIKYMOmnRVX5ZrYG2zYtJN7zzvZHb6ZZV5bJIDyef2V6vSX+GavmbWLzCeASvP6BUy3Kq9v9ppZu8j8PYBK8/qn6/xzHfLNXjNrG5lPANWGdA5ZlNv3fU9Xjg1nnuCxfzNrG5kfAppuzL+vp8uLtZhZW8v8FYArdZqZVZZKApB0pqQdkvZK6m/kZ51+Up/n9ZuZVZDWENCDwCrg8/PxYa7UaWa2v1QSQEQ8BCApjY83MzNAEdNNipyHD5eGgY9GxOYq+6wGVgP09vYu27hxY9Vjjo2N0d3dnWSYqclSWyBb7clSWyBb7XFb9jc4OLglIvYfbo+IhnwBd1IY6in/es+kfYaB/lqPuWzZspjJ3XffPeM+rSJLbYnIVnuy1JaIbLXHbdkfsDkq9KkNGwKKiLc16thmZla/zE8DNTOzytKaBvpeSY8CbwFulbQpjTjMzNpZqjeBZ0vSk8BPZtjtMOBn8xDOfMhSWyBb7clSWyBb7XFb9veqiDi8fGNLJYBaSNocle52t6AstQWy1Z4stQWy1R63pXa+B2Bm1qacAMzM2lQWE8DlaQeQoCy1BbLVniy1BbLVHrelRpm7B2BmZrXJ4hWAmZnVwAnAzKxNZTIBSNog6XuSHpB0o6SetGOaq/lcO6FRJJ0maaekH0o6L+146iHpi5KekPRg2rHUS9LRku6W9FDx39gfpx1TPSQtlPRtSduK7bko7ZjqJalT0oikWxpx/EwmAOAO4PUR8Qbg+8D5KcdTj9LaCfekHchcSOoE/hZ4O3AccLak49KNqi5fAk5LO4iE7AbWRMSvAMuB/9niv5sXgJMj4gTgROA0SctTjqlefww81KiDZzIBRMTtEbG7+ON9wFFpxlOPiHgoInamHUcd3gT8MCJ+FBEvAhuB96Qc05xFxD3Az9OOIwkR8XhEfKf4/bMUOpqWXTmpWPhyrPhjrvjVsrNcJB0FvBP4h0Z9RiYTQJn/CtyWdhBtrA94ZNLPj9LCnUxWSVoMnATcn24k9SkOmWwFngDuiIhWbs9lwMeAvY36gLSWhKybpDuBIyq89PGI+L/FfT5O4TL3yvmMbbZqaUsLq7TsW8uelWWRpG7geuAjEfFM2vHUIyL2ACcW7/vdKOn1EdFy92skvQt4IiK2SBpo1Oe0bAKYab0BSb8LvAv4zWjyhx0yvnbCo8DRk34+CngspVisjKQchc7/yoi4Ie14khIRo8UVB0+jcB+t1awAVkp6B7AQOLi11GgAAAM3SURBVFjSFRFxTpIfkskhIEmnAX8KrIyI59OOp839K/BaSa+WtAB4P3BTyjEZoMKi3F8AHoqIT6cdT70kHV6a8SepC3gb8L10o5qbiDg/Io6KiMUU/mbuSrrzh4wmAOBzwEHAHZK2Svq7tAOaq1ZfO6F4M/6DwCYKNxmviYgd6UY1d5KuAr4FLJH0qKQ/SDumOqwAfgc4ufh3srV4xtmqXgncLekBCiced0REQ6ZPZoVLQZiZtamsXgGYmdkMnADMzNqUE4CZWZtyAjAza1NOAGZmbcoJwDJF0p7idMYdxaqQ50rqKL7WL+lvUorrXxI6TstXh7Xm4WmglimSxiKiu/j9K4CvAvdGxIXpRpYMSb9CoTbM54GPRsTmlEOyFuYrAMusiHgCWA18UAUDpbrqktZJ+rKk2yX9WNIqSf9L0nZJXy+WSEDSMknfkLRF0iZJryxuH5b0qWL9+e9L+rXi9uOL27YW16N4bXH7WPG/Kq5X8WDxs84qbh8oHvM6FdayuLL4pG55m1q9Oqw1EScAy7SI+BGFf+evqPDysRTK7b4HuAK4OyKWAuPAO4tJ4LPAGRGxDPgicPGk9x8QEW8CPgKUrjD+G/CZiDgR6KdQC2myVRRq1Z9AoVTBhlJSoVCN8yMU1k14DYUndc0apmWLwZnNQqWKpAC3RcSEpO1AJ/D14vbtwGJgCfB6CiVFKO7z+KT3l4qnbSnuD4UyER8v1nK/ISJ+UPaZbwWuKlat3CXpG8AbgWeAb0fEowDFksaLgW/OtrFmtfIVgGWapNcAeyjUhy/3AkBE7AUmJlWN3Uvh5EjAjog4sfi1NCJOKX9/8fgHFI/1VWAlhauITZJOLg+pSrgvTPp+3zHNGsUJwDJL0uHA3wGfm2NJ8J3A4ZLeUjxeTtLxM3zma4AfRcTfUKh6+oayXe4BziouXHI48OvAt+cQm1ndnAAsa7pK00CBO4HbgTktDl5cwvIM4FOStgFbgV+d4W1nAQ8Wh3BeB/xT2es3Ag8A24C7gI9FxL/XGlOrV4e15uJpoGZmbcpXAGZmbcoJwMysTTkBmJm1KScAM7M25QRgZtamnADMzNqUE4CZWZv6/1S92wNkvTGKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(sample[:,0], sample[:,1])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a simple dataset with multivariate Gaussian errors, we do not need an optimizer to find the least squares vector solution, we can just compute it directly. This least squares value can be used to compare solutions computed with optimizers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00161585 2.00351846]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sample, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4-1-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-1-1:** With the simulated data prepared, it is time to try gradient decent! You will implement the **batch gradient decent** algorithm to minimize the least squares error between an estimate of the multivariate mean and the data values. This algorithm is considered batch gradient decent since all of the cases are used to compute each update of the gradient. Now, do the following: \n",
    "> 1. Complete the `compute_gradient` function. The gradient is the normalized vector (multivariate) mean difference between an estimate of the minimum and the data values. In this case, the gradient is 2-dimensional. The gradient is computed by the following steps:     \n",
    "  a. The normalization factor is computed as $2.0/N$, where $N$ is the number of observations.    \n",
    "  b. The vector difference between the observations and the current estimate is computed using the [numpy.subtract](https://numpy.org/doc/stable/reference/generated/numpy.subtract.html) function.     \n",
    "  c. The 2-d vector sum of the differences is computed, multiplied by the normalization factor is returned. This result is the current estimate of the gradient vector.       \n",
    "> 2. The work of the gradient descent algorithm is done in the `while` loop. The termination condition is the l2 norm of the gradient (magnitude of the gradient) is less than a set value or the maximum number of iterations has been executed. The learning rate is fixed for each optimization step. A list is created of the estimates of the minimum and the error is compute as the standard deviation of the gradient. The while loop terminates when either the error becomes less than the stopping criteria, or the maximum number of iterations is reached. To complete the code in the `while` loop do the following:   \n",
    "  a. The gradient vector is computed using the the `compute_gradient` function.  \n",
    "  b. The vector valued current `estimate` is updated by adding the gradient vector times the learning rate. \n",
    "> 3. Execute your code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 23\n",
      "Final gradient magnitude = 0.009240920576144982\n",
      "MLE = [[0.99422523 1.98873511]]\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(x, estimate):\n",
    "    ## Your code goes below\n",
    "    normalization = 2.0/x.shape[0]\n",
    "    diff = np.subtract(x, estimate)\n",
    "    gradient = normalization * np.sum(diff, axis = 0)\n",
    "    return gradient\n",
    "\n",
    "def grad_decent(x, estimate, lr, stopping, max_its = 100):\n",
    "    out = estimate\n",
    "    out = out.reshape((1,2))\n",
    "    err = 10000000.0 ## starting criteria for graident metric\n",
    "    i = 1\n",
    "    while(err > stopping and i < max_its):\n",
    "        ## Your code goes below\n",
    "        gradient = compute_gradient(x, estimate)\n",
    "        estimate = estimate + lr * gradient\n",
    "        out = np.append(out, estimate.reshape((1,2)))\n",
    "        err = np.std(gradient)\n",
    "        i = i + 1\n",
    "    out = out.reshape((i, 2))    \n",
    "    print('Number of iterations = ' + str(i))   \n",
    "    print('Final gradient magnitude = ' + str(err))\n",
    "    print('MLE = ' + str(out[i-1:]))\n",
    "    return out\n",
    "\n",
    "lr = 0.1\n",
    "stopping = 0.01\n",
    "#start = np.array([5.0,-1.0])\n",
    "start = np.array([0.0,0.0])\n",
    "nr.seed(4566)\n",
    "steps = grad_decent(sample, start, lr, stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In one or two sentences answer the following questions: How can you describe the convergence of this optimizer?     \n",
    "\n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next execute the code in the cell below to visualize the trajectory taken by the optimizer.  The red points in the plot show the solutions found a each step of the gradient decent algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_decent(x, steps):\n",
    "    plt.scatter(x[:,0], x[:,1])\n",
    "    plt.scatter(steps[:,0], steps[:,1], color = 'red')\n",
    "\n",
    "plot_decent(sample, steps)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path of convergence looks good. You can see that the rate of convergence of each optimization step decreases as the algorithm approaches convergence. This is expected, since the gradient is decreasing as the optimizer converges.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Stochastic gradient decent\n",
    "\n",
    "The **stochastic gradient decent (SGD)** algorithm (Nemirovshi and Yudin, 1978) is the workhorse of deep neural network training. As opposed to batch gradient decent, SGD computes the expected gradient using a **mini-batch** Bernoulli sampled from the full set of cases. Mini-batch optimization is often referred to as **online optimization** since the optimizer algorithm can update the solution as cases arrive. \n",
    "\n",
    "The basic idea of stochastic optimization is using a Bernoulli random sample of the data to estimate the **expected value** of the weights. The weight update for SGD then becomes:\n",
    "\n",
    "$$W_{t+1} = W_t + \\alpha\\ E_{\\hat{p}data}\\Big[ \\nabla_{W} J(W_t) \\Big]$$ \n",
    "\n",
    "where,  \n",
    "$E_{\\hat{p}data} \\big[ \\big]$ is the expected value of the gradient given the Bernoulli sample of the data $\\hat{p}data$.\n",
    "\n",
    "Since the SGD algorithm works on mini-batches, it is highly scalable when compared to the batch gradient decent. The later must keep all cases in memory.  \n",
    "\n",
    "Choosing batch size can require some tuning. If the batch is too small, the gradient estimate will be poor. Further, hardware resources will not be fully utilized. Large batches require significant memory. Further, large batches can slow down the computation of each gradient step.  \n",
    "\n",
    "Empirically, SGD has good convergence properties. This behavior seems to arise since the mini-batch samples provide a better exploration of the loss function space. It seems to be the case that the variations in the gradient from one mini-batch sample to another help the algorithm escape from saddle points or other areas of the loss function with poor convergence properties. In fact, for very large datasets, the SGD algorithm often converges before the first pass through the data is completed. \n",
    "\n",
    "The pseudo code for the SGD algorithm is:\n",
    "\n",
    "`\n",
    "Random_sort(cases)\n",
    "while(grad > stopping_criteria):\n",
    "    mini-batch = sample_next_n(cases)\n",
    "    grad = compute_expected_grad(mini_batch)\n",
    "    weights = update_weights(weights, grad)`\n",
    "    \n",
    "Notice that if the sampling continues for more than one cycle through the cases, the samples are biased. In practice, this small bias does not seem to mater much.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Example of basic SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4-1-2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-1-2:** You will now complete the code in the cell below to create a basic SGD algorithm. The code is nearly identical to the batch gradient decent function. You can use the `compute_gradient` function you have already created.    \n",
    "> Production SDG code randomly sorts the data samples and then loops over these sample. To keep things simple here you will Bernoulli sample of the full dataset within the `while` loop by these steps, which are the only difference from the batch gradient descent code.   \n",
    "> 1. Using [numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) take a Bernoulli sample of size `batch_size` from the vector `indx`.      \n",
    "> 2. Subsample the full dataset using your index sample in your call to `compute_gradient`. \n",
    "> 3. Execute your code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 25\n",
      "Final gradient norm = 0.007873996213824563\n",
      "MLE = [[1.09062416 2.08478185]]\n"
     ]
    }
   ],
   "source": [
    "def sgd(x, estimate, lr, stopping, batch_size = 8, max_its = 100):\n",
    "    out = estimate\n",
    "    out = out.reshape((1,2))\n",
    "    err = 10000000.0 # stopping criteria for graident norm\n",
    "    i = 1\n",
    "    indx = range(x.shape[0])\n",
    "    while((err > stopping) and (i < max_its)):\n",
    "        ## Your code goes below\n",
    "        sample = nr.choice(indx, batch_size)\n",
    "        gradient = compute_gradient(x[sample,:], estimate)\n",
    "        estimate = estimate + lr * gradient\n",
    "        out = np.append(out, estimate.reshape((1,2)))\n",
    "        err = np.std(gradient)\n",
    "        i = i + 1\n",
    "    out = out.reshape((i, 2))    \n",
    "    print('Number of iterations = ' + str(i))   \n",
    "    print('Final gradient norm = ' + str(err))\n",
    "    print('MLE = ' + str(out[i-1:]))\n",
    "    return out\n",
    "\n",
    "lr = 0.1\n",
    "stopping = 0.01\n",
    "#start = np.array([5.0,-1.0])\n",
    "start = np.array([0.0,0.0])\n",
    "nr.seed(5556)\n",
    "steps = sgd(sample, start, lr, stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare this result to those obtained by the batch gradient descent algorithm and answer the following questions in one or two sentences.   \n",
    "> 1. Is the convergence of the SGD algorithm comparable to the batch gradient decent algorithm, and why?     \n",
    "> 2. Is there some evidence of stochastic error in the result?        \n",
    "\n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.      \n",
    "> 2.     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to visualize the optimization trajectory. The red points in the plot show the solutions found using each mini-batch in the steps of the SGD algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decent(sample, steps)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the trajectory of the SGD optimizer to the batch gradient decent optimizer. In appears that th SGD optimizer converges faster initially, but then seems to wander a bit near convergence. This makes sense, since the expected gradient from the small mini-batches is likely to be noisier than the batch gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adding momentum to SGD\n",
    "\n",
    "With a poorly conditioned loss function, SGD is known to 'zig-zag' back and forth as the optimizer moves toward convergence. This problem can be severe in some case, leading to many wasted optimization step that provide only minimal reduction in the loss function at best. To overcome this problem, in  1988 paper Rummelhart, et. al., proposed adding a **momentum** term to the gradient update. \n",
    "\n",
    "Recall from Newtonian mechanics that $momentum = m \\cdot v$, where $m$ is the mass and \n",
    "\n",
    "$v$ is the velocity   \n",
    "\n",
    ". If we assume that $m = 1$ then momentum is the same as velocity. The model weight update then becomes a weighted sum of velocity (momentum) and the gradient:\n",
    "\n",
    "$$v^{(l)} = momentum \\cdot v^{(l - 1)} + lr \\cdot \\nabla_{W} J(W^{(l)})\\\\\n",
    "W^{(l+1)} = W^{(l)} + v^{(l)}$$   \n",
    "where,   \n",
    "$v^{(l)}$ is the velocity at step $l$,    \n",
    "$momentum$ is the momentum multiplier,    \n",
    "$lr$ is the learning rate.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4-1-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-1-3:** You will complete the code in the cell below by the following steps to add momentum to the gradient update. \n",
    "> 1. The first two lines of code are identical to the SGD algorithm for computing the stochastically sampled gradient.   \n",
    "> 2. Compute the momentum update using the gradient, learning rate and current value of the momentum.  \n",
    "> 3. Updated the estimate of the minimum by adding the momentum to the current estimate. \n",
    "> 4. Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 17\n",
      "Final gradient value = 0.005270385456672952\n",
      "MLE = [[0.92872426 1.91667771]]\n"
     ]
    }
   ],
   "source": [
    "def sgd_momentum(x, estimate, lr, stopping, momentum, batch_size = 8, max_its = 100):\n",
    "    out = estimate\n",
    "    out = out.reshape((1,2))\n",
    "    v = np.zeros((1, x.shape[1]))\n",
    "    err = 10000000.0 ## starting criteria for graident metric\n",
    "    i = 1\n",
    "    indx = range(x.shape[0])\n",
    "    while((err > stopping) and (i < max_its)):\n",
    "        ## Your code goes below\n",
    "        sample = nr.choice(indx, batch_size)\n",
    "        gradient = compute_gradient(x[sample,:], estimate)\n",
    "        v = momentum * v + lr * gradient\n",
    "        estimate = estimate + v        \n",
    "        out = np.append(out, estimate.reshape((1,2)))\n",
    "        err = np.std(gradient)\n",
    "        i = i + 1\n",
    "    out = out.reshape((i, 2))    \n",
    "    print('Number of iterations = ' + str(i))   \n",
    "    print('Final gradient value = ' + str(err))\n",
    "    print('MLE = ' + str(out[i-1:]))\n",
    "    return out\n",
    "\n",
    "lr = 0.1\n",
    "stopping = 0.01\n",
    "#start = np.array([5.0,-1.0])\n",
    "start = np.array([0.0,0.0])\n",
    "momentum = 0.1\n",
    "nr.seed(456)\n",
    "steps = sgd_momentum(sample, start, lr, stopping, momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare these results to those obtained with basic SGD (no momentum) and answer these question in one or two sentences.    \n",
    "> 1. How has the convergence rate changed with the addition of the momentum term to the update.     \n",
    "> 2. Is the stochastic error similar to the basic SGD results. \n",
    "\n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**  \n",
    "> 1.         \n",
    "> 2.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the code below to display and examine the trajectory of the optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_decent(sample, steps)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is largely the same as for the basic SGD algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 SGD with Keras\n",
    "\n",
    "Keras has an extensive library of optimizers, including a full-featured SGD method. The Keras website has somewhat sparse [documentation on the available optimizers](https://keras.io/optimizers/), with references for some of the algorithms.\n",
    "\n",
    "As a first step before trying out the Keras SGD optimizer we need to create test and training data set in the form of numpy arrays. Execute the code in the cell below that does just this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = range(sample.shape[0])\n",
    "nr.seed(9988)\n",
    "indx = ms.train_test_split(indx, test_size = 100)\n",
    "x_train = np.ravel(sample[indx[0],[0]])\n",
    "y_train = np.ravel(sample[indx[0],[0]])\n",
    "x_test = np.ravel(sample[indx[1],[1]])\n",
    "y_test = np.ravel(sample[indx[1],[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data prepared with can get to work with training and testing the neural network model with the SGD optimizer. To create a problem where a neural network can be applied, we will solve the regression problem for the simulated data we have been using. \n",
    "\n",
    "The SGD optimizer in Keras has a number of arguments including for: \n",
    "- learning rate: `lr`, \n",
    "- gradient clipping: `clipnorm`,\n",
    "- decay rate: `decay`, \n",
    "- momentum: `momentum`. \n",
    "\n",
    "Examine the code below for details. Execute the code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    '''Function to plot the loss vs. epoch'''\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(test_loss) + 1))\n",
    "    plt.plot(x, test_loss, color = 'red')\n",
    "    plt.plot(x, train_loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-15d44e662449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m nn.add(layers.Dense(128, activation = 'relu', input_shape = (1, ),\n\u001b[1;32m---> 16\u001b[1;33m                         kernel_regularizer=regularizers.l2(0.01)))\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m nn.add(layers.Dense(128, activation = 'relu',\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 raise ValueError('Layer ' + self.name + ' was called with '\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \"\"\"\n\u001b[1;32m--> 695\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m    697\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_TensorLike\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'"
     ]
    }
   ],
   "source": [
    "## First define the layers of the regression model. \n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, activation = 'relu', input_shape = (1, ),\n",
    "                        kernel_regularizer=regularizers.l2(0.01)))\n",
    "nn.add(Dropout(0.5))\n",
    "nn.add(layers.Dense(128, activation = 'relu',\n",
    "                        kernel_regularizer=regularizers.l2(0.01)))\n",
    "nn.add(layers.Dense(1))\n",
    "\n",
    "## Define the SGD optimizer\n",
    "sgd = optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.5, nesterov=False)\n",
    "## The optimizer is used at the compile stage\n",
    "nn.compile(optimizer = sgd, loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "## Define the callback list\n",
    "filepath = 'my_model_file.hdf5' # define where the model is saved\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "        patience = 1 # Stop after one step with lower accuracy\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = filepath, # file where the checkpoint is saved\n",
    "        monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "        save_best_only = True # Only save model if it is the best\n",
    "    )\n",
    "]\n",
    "\n",
    "## Now fit the model\n",
    "start = time.time() ## Get the system time at strat of execution\n",
    "history = nn.fit(x_train, y_train, \n",
    "                  epochs = 40, batch_size = 1,\n",
    "                  validation_data = (x_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "end = time.time() ## Get the system time at the end of execution\n",
    "## Execution time is the difference between the end and start times\n",
    "print('Execution time = ' + str(end - start))\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that training loss continues to decrease even after test loss increases. This is a commonly observed behavior when training neural networks. The optimizer continues to reduce the training loss, even after the model is over fit.   \n",
    "\n",
    "We should check that the learned model actually makes sense. The code in the cell below predicts score values for the test dataset, prints the RMSE and plots the result. Execute this code and examine the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-394889810ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mplot_reg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# Note: in this case, `any` and `all` are equivalent since we disallow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;31m# mixed symbolic/value inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# Note: in this case, `any` and `all` are equivalent since we disallow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;31m# mixed symbolic/value inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_TensorLike\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'"
     ]
    }
   ],
   "source": [
    "def plot_reg(x, y_score, y):\n",
    "    ax = plt.figure(figsize=(6, 6)).gca() # define axis\n",
    "    \n",
    "    ## Get the data in plot order\n",
    "    xy = sorted(zip(x,y_score))\n",
    "    x = [x for x, _ in xy]\n",
    "    y_score = [y for _, y in xy]\n",
    "\n",
    "    ## Plot the result\n",
    "    plt.plot(x, y_score, c = 'red')\n",
    "    plt.scatter(x, y)\n",
    "\n",
    "predicted = nn.predict(x_test)\n",
    "plot_reg(x_test, predicted, y_test)\n",
    "print(np.std(predicted - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem reasonable given the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Adaptive gradient decent algorithms\n",
    "\n",
    "Up until now, we have been worked with algorithms with constant learning rates. In many cases, the gradient of the loss function will change multiple times before convergence is achieved. For example, the gradient may decrease and then increase again. In these cases, a constant learning rate results in slow convergence. There are several possible approaches to changing learning rates of optimization algorithms. \n",
    "\n",
    "One simple approach is use **learning rate decay**. The learning rate decays from a starting value and decreases as the optimization proceeds. This approach is effective in cases where the gradient decreases fairly steadily as the optimization proceeds. The lower learning rate reduces the chance that the algorithm over-shoots the optimum point and then wanders around with slow convergence. We have seen this behavior in the foregoing SGD examples.    \n",
    "\n",
    "The second approach is to use algorithms with an **adaptive learning rate**. As the name implies, adaptive learning rate algorithms change their rate of convergence depending on the gradient. Ideally, if the learning rate should increase when plateaus and poorly conditioned areas of the loss function are encountered. The learning rate should decrease when the gradient of the loss function is better behaved. In practice, these ideals are hard to achieve and researchers have created many algorithms using various heuristics to adapt learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 An example of adaptive learning, Adam\n",
    "\n",
    "The Adam algorithm (Kingma and Ba, 2014) uses a fairly complicated set of huristics adapt the learning rate. Adam uses both first and second order momentum measures. Second order momentum is analogous to kinetic energy in Newtonian mechanics. Further, Adam encorporates exponential decay in both momentum measures to ensure that more recent values dominate the learning rate updates. \n",
    "\n",
    "The code in the cell below implements a basic version of Adam. Examine this code for details, execute this code and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(x, estimate, lr, stopping, momentum, ke, batch_size = 32, max_its = 1000):\n",
    "    out = estimate\n",
    "    out = out.reshape((1,2))\n",
    "    s = np.zeros((1, x.shape[1]))\n",
    "    r = np.zeros((1, x.shape[1]))\n",
    "    grad_norm = 10000000.0 ## starting criteria for graident metric\n",
    "    i = 1\n",
    "    indx = range(x.shape[0])\n",
    "    while((grad_norm > stopping) and (i < max_its)):\n",
    "        sample_idx = nr.choice(indx, batch_size)\n",
    "        grad = compute_gradient(x[sample_idx,:], estimate)\n",
    "        s = momentum * s + (momentum - 1.0) * grad\n",
    "        s_tilde = s/(1 - momentum**i)\n",
    "        r = ke * r + (ke - 1.0) * np.multiply(grad, grad)\n",
    "        r_tilde = np.sqrt(np.abs(r/(1 - ke**i)))\n",
    "        delta = np.array([lr* ss/(rr + 0.000001) for ss, rr in zip(s_tilde, r_tilde)])\n",
    "        estimate = estimate - delta \n",
    "        out = np.append(out, estimate.reshape((1,2)))\n",
    "        grad_norm = np.std(grad) \n",
    "        i = i + 1\n",
    "    out = out.reshape((i, 2))    \n",
    "    print('Number of iterations = ' + str(i))   \n",
    "    print('Final gradient value = ' + str(np.std(grad)))\n",
    "    print('MLE = ' + str(out[i-1:]))\n",
    "    return out\n",
    "\n",
    "lr = 0.2\n",
    "stopping = 0.01\n",
    "#start = np.array([5.0,-1.0])\n",
    "start = np.array([0.0,0.0])\n",
    "momentum = 0.1\n",
    "ke = 0.2\n",
    "nr.seed(458)\n",
    "steps = adam(sample, start, lr, stopping, momentum, ke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are not too different from the SGD algorithms. \n",
    "\n",
    "Now, exectute the code below and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decent(sample, steps)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4-1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 4-1-4:** In one or two sentences answer the following questions.   \n",
    "> 1. Compare the trajectory of the Adam optimizer to SGD with momentum.      \n",
    "> 2. How does the stochastic error compare to the SGD algorithms, and why.  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.       \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Adaptive optimization with Keras\n",
    "\n",
    "Now, let's try adaptive optimization with Keras. We will use one of the mostly widely used adaptive algorithms, RMSprop (Hinton, 2012). Like Adam, RMSprop accumulates a measure of the squared gradient to change the learning rate. An exponential decay is applied to the accumulated squared gradient to ensure that more recent experience dominates the learning rate. \n",
    "\n",
    "Examine the code below for details. Excute the code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First define the regression model. \n",
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(128, activation = 'relu', input_shape = (1, ),\n",
    "                        kernel_regularizer=regularizers.l2(0.01)))\n",
    "nn.add(Dropout(0.5))\n",
    "nn.add(layers.Dense(128, activation = 'relu',\n",
    "                        kernel_regularizer=regularizers.l2(0.01)))\n",
    "nn.add(layers.Dense(1))\n",
    "\n",
    "## Define the RMS optimizer\n",
    "RMS = optimizers.RMSprop(learning_rate=0.01)\n",
    "nn.compile(optimizer = RMS, loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "## Now fit the model\n",
    "start = time.time() # The time as execution start\n",
    "history = nn.fit(x_train, y_train, \n",
    "                  epochs = 40, batch_size = 1,\n",
    "                  validation_data = (x_test, y_test),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 0)\n",
    "end = time.time() # Time at execution endf\n",
    "print('Execution time = ' + str(end - start))\n",
    "\n",
    "## Visualize the outcome\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that RMSprop converges in fewer epochs than SGD for this situation. The same type of over-fitting of the model is also evident. \n",
    "\n",
    "Once again, we should check that the learned model actually makes sense. The code in the cell below predicts score values for the test dataset, prints the RMSE and plots the result. Execute this code and examine the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = nn.predict(x_test)\n",
    "plot_reg(x_test, predicted, y_test)\n",
    "print(np.std(predicted - y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is similar to the one achieved with SGD, but perhaps a bit better and faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5.0 Weight initial values\n",
    "\n",
    "When training deep neural networks the initial values chosen for the weights can have a significant effect on the results. If weights are all set to the same initial value several possible problems will arise:\n",
    "- Some of the weights may be linearly dependent. In this case, some weights will change together during training and not be correctly learned. \n",
    "- Some weights will become **stuck** at the initial value and are never learned. This is special case of the first problem, for the most part. \n",
    "\n",
    "Fortunately, the solution to this problem is simple; **randomize** the starting values of the weights.  This process is sometimes referred to as adding **fuzz** to the initial weights. A number of schemes have been tried. For example, initial weight values can be drawn from a Gaussian or Normal distribution. In practice, drawing the initial values from a Uniform distribution works as well as any other scheme.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2018, 2019, 2022, Stephen  F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
